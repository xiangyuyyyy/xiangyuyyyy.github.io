1:"$Sreact.fragment"
2:I[3719,["17","static/chunks/17-9c04c9413a9f9f1f.js","874","static/chunks/874-6cc630662f3664af.js","862","static/chunks/862-15038af301665bb3.js","177","static/chunks/app/layout-566a46c7d7b3215f.js"],"ThemeProvider"]
3:I[768,["17","static/chunks/17-9c04c9413a9f9f1f.js","874","static/chunks/874-6cc630662f3664af.js","862","static/chunks/862-15038af301665bb3.js","177","static/chunks/app/layout-566a46c7d7b3215f.js"],"default"]
4:I[7555,[],""]
5:I[1295,[],""]
6:I[2548,["17","static/chunks/17-9c04c9413a9f9f1f.js","874","static/chunks/874-6cc630662f3664af.js","862","static/chunks/862-15038af301665bb3.js","177","static/chunks/app/layout-566a46c7d7b3215f.js"],"default"]
7:I[7437,["17","static/chunks/17-9c04c9413a9f9f1f.js","178","static/chunks/178-595a94b9af1e67b5.js","874","static/chunks/874-6cc630662f3664af.js","748","static/chunks/748-1f3129a1e6365cf9.js","974","static/chunks/app/page-fb1a19f5557a7b94.js"],"default"]
8:I[9507,["17","static/chunks/17-9c04c9413a9f9f1f.js","178","static/chunks/178-595a94b9af1e67b5.js","874","static/chunks/874-6cc630662f3664af.js","748","static/chunks/748-1f3129a1e6365cf9.js","974","static/chunks/app/page-fb1a19f5557a7b94.js"],"default"]
9:I[5218,["17","static/chunks/17-9c04c9413a9f9f1f.js","178","static/chunks/178-595a94b9af1e67b5.js","874","static/chunks/874-6cc630662f3664af.js","748","static/chunks/748-1f3129a1e6365cf9.js","974","static/chunks/app/page-fb1a19f5557a7b94.js"],"default"]
f:I[1990,["17","static/chunks/17-9c04c9413a9f9f1f.js","178","static/chunks/178-595a94b9af1e67b5.js","874","static/chunks/874-6cc630662f3664af.js","748","static/chunks/748-1f3129a1e6365cf9.js","974","static/chunks/app/page-fb1a19f5557a7b94.js"],"default"]
10:I[9665,[],"MetadataBoundary"]
12:I[9665,[],"OutletBoundary"]
15:I[4911,[],"AsyncMetadataOutlet"]
17:I[9665,[],"ViewportBoundary"]
19:I[6614,[],""]
:HL["/_next/static/css/19bf7477d82f620e.css","style"]
a:T50d,@article{yangvisual,
  title = {A Visual Annotation-Free Method That Rivals Fully Supervised Methods for Grounded Multimodal Named Entity Recognition},
  author = {Yang, Jia and Yu, Jianfei and Du, Zilin and Wang, Wenya and Yang, Li and Xia, Rui and Li, Boyang},
  abstract = {Grounded Multimodal Named Entity Recognition (GMNER) aims to extract named entities, their types, and corresponding visual objects from image-text pairs. However, existing GMNER methods rely on costly multimodal annotations, limiting their scalability in real applications. To address this issue, we propose a visual annotation-free framework that leverages text-only NER data and a Zero-shot Entity Visual Grounding (ZeroEVG) approach. ZeroEVG consists of three modules: (1) Candidate Object Generation, which pre-selects visual object candidates; (2) Entity-Object Matching, which determines whether an entity has a visual presence; and (3) Entity Visual Localization, which employs a variant of GradCAM to identify bounding boxes for groundable entities. Experimental results on two benchmark datasets show that our visual annotationfree framework achieves competitive performance with fully supervised multimodal approaches, and even surpasses some of them under the same backbone on both GMNER and EVG tasks.}
}b:T5a5,Aspect‐Based Sentiment Analysis (ABSA) has evolved from textual analysis to a multimodal paradigm, integrating visual information to capture nuanced sentiments. Despite advancements, existing Multimodal ABSA (MABSA) research remains limited in granularity, which focuses on either coarse‐level categories or named entities, neglecting fine‐grained sentiment analysis at the aspect term level and visual objects depicted in images. To address these gaps, we propose a new task, Multimodal Aspect‐Category‐Sentiment‐Appearance Quad Extraction (MASQE), which aims to extract textual aspect terms and visual aspect objects, their associated categories, sentiments, and modality appearances. To facilitate research on this task, we introduce MM‐Rest, a novel dataset comprising 19,962 manually annotated aspect‐category‐sentiment‐appearance quadruples from restaurant reviews, annotated across both text and images. Additionally, we propose a Visual Aspect‐aware Multimodal Large Language Model (VAM‐LLM), which leverages predicted visual aspect objects to enhance multimodal quadruple extraction in an end‐to‐end framework. Experimental results demonstrate the effectiveness of VAM‐LLM over baseline systems, establishing strong benchmarks for MASQE and its subtasks. We believe our work opens new avenues for fine‐grained multimodal sentiment analysis, providing rich resources and methodologies for future research.c:T71b,@article{yang2025annotating,
  title = {Annotating aspects in text and image: A new task and dataset for multimodal aspect-based sentiment analysis},
  author = {Yang, Li and Yang, Jia and Na, Jin-Cheon and Yu, Jianfei},
  journal = {Journal of the Association for Information Science and Technology},
  year = {2025},
  publisher = {Wiley Online Library},
  abstract = {Aspect‐Based Sentiment Analysis (ABSA) has evolved from textual analysis to a multimodal paradigm, integrating visual information to capture nuanced sentiments. Despite advancements, existing Multimodal ABSA (MABSA) research remains limited in granularity, which focuses on either coarse‐level categories or named entities, neglecting fine‐grained sentiment analysis at the aspect term level and visual objects depicted in images. To address these gaps, we propose a new task, Multimodal Aspect‐Category‐Sentiment‐Appearance Quad Extraction (MASQE), which aims to extract textual aspect terms and visual aspect objects, their associated categories, sentiments, and modality appearances. To facilitate research on this task, we introduce MM‐Rest, a novel dataset comprising 19,962 manually annotated aspect‐category‐sentiment‐appearance quadruples from restaurant reviews, annotated across both text and images. Additionally, we propose a Visual Aspect‐aware Multimodal Large Language Model (VAM‐LLM), which leverages predicted visual aspect objects to enhance multimodal quadruple extraction in an end‐to‐end framework. Experimental results demonstrate the effectiveness of VAM‐LLM over baseline systems, establishing strong benchmarks for MASQE and its subtasks. We believe our work opens new avenues for fine‐grained multimodal sentiment analysis, providing rich resources and methodologies for future research.}
}d:T573,As an important task in multimodal information extraction, Multimodal Named Entity Recognition (MNER) has recently attracted considerable attention. One key challenge of MNER lies in the lack of sufficient fine-grained annotated data, especially in low-resource scenarios. Although data augmentation is a widely used technique to tackle the above issue, it is challenging to simultaneously generate synthetic text-image pairs and their corresponding high-quality entity annotations. In this work, we propose a novel Generative Multimodal Data Augmentation (GMDA) framework for MNER, which contains two stages: Multimodal Text Generation and Multimodal Image Generation. Specifically, we first transform each annotated sentence into a linearized labeled sequence, and then train a Label-aware Multimodal Large Language Model (LMLLM) to generate the labeled sequence based on a label-aware prompt and its associated image. We further employ a Stable Diffusion model to generate the synthetic images that are semantically related to these sentences. Experimental results on three benchmark datasets demonstrate the effectiveness of the proposed GMDA framework, which consistently boosts the performance of several competitive methods for two subtasks of MNER in both full-supervision and low-resource settings. The low-resource dataset and source code are released at https://github.com/NUSTM/GMDA.e:T6ec,@inproceedings{li2024generative,
  title = {Generative Multimodal Data Augmentation for Low-Resource Multimodal Named Entity Recognition},
  author = {Li, Ziyan and Yu, Jianfei and Yang, Jia and Wang, Wenya and Yang, Li and Xia, Rui},
  booktitle = {Proceedings of the 32nd ACM International Conference on Multimedia},
  pages = {7336--7345},
  year = {2024},
  abstract = {As an important task in multimodal information extraction, Multimodal Named Entity Recognition (MNER) has recently attracted considerable attention. One key challenge of MNER lies in the lack of sufficient fine-grained annotated data, especially in low-resource scenarios. Although data augmentation is a widely used technique to tackle the above issue, it is challenging to simultaneously generate synthetic text-image pairs and their corresponding high-quality entity annotations. In this work, we propose a novel Generative Multimodal Data Augmentation (GMDA) framework for MNER, which contains two stages: Multimodal Text Generation and Multimodal Image Generation. Specifically, we first transform each annotated sentence into a linearized labeled sequence, and then train a Label-aware Multimodal Large Language Model (LMLLM) to generate the labeled sequence based on a label-aware prompt and its associated image. We further employ a Stable Diffusion model to generate the synthetic images that are semantically related to these sentences. Experimental results on three benchmark datasets demonstrate the effectiveness of the proposed GMDA framework, which consistently boosts the performance of several competitive methods for two subtasks of MNER in both full-supervision and low-resource settings. The low-resource dataset and source code are released at https://github.com/NUSTM/GMDA.}
}0:{"P":null,"b":"JneXPOaQwBGqK3dxEnceo","p":"","c":["",""],"i":false,"f":[[["",{"children":["__PAGE__",{}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/19bf7477d82f620e.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","className":"scroll-smooth","suppressHydrationWarning":true,"children":[["$","head",null,{"children":[["$","link",null,{"rel":"icon","href":"/fig.png","type":"image/svg+xml"}],["$","link",null,{"rel":"dns-prefetch","href":"https://google-fonts.jialeliu.com"}],["$","link",null,{"rel":"preconnect","href":"https://google-fonts.jialeliu.com","crossOrigin":""}],["$","link",null,{"rel":"preload","as":"style","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap"}],["$","link",null,{"rel":"stylesheet","id":"gfonts-css","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap","media":"print"}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              (function(){\n                var l = document.getElementById('gfonts-css');\n                if (!l) return;\n                if (l.media !== 'all') {\n                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });\n                }\n              })();\n            "}}],["$","noscript",null,{"children":["$","link",null,{"rel":"stylesheet","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap"}]}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              try {\n                const theme = localStorage.getItem('theme-storage');\n                const parsed = theme ? JSON.parse(theme) : null;\n                const setting = parsed?.state?.theme || 'system';\n                const prefersDark = typeof window !== 'undefined' && window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;\n                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));\n                var root = document.documentElement;\n                root.classList.add(effective);\n                root.setAttribute('data-theme', effective);\n              } catch (e) {\n                var root = document.documentElement;\n                root.classList.add('light');\n                root.setAttribute('data-theme', 'light');\n              }\n            "}}]]}],["$","body",null,{"className":"font-sans antialiased","children":["$","$L2",null,{"children":[["$","$L3",null,{"items":[{"title":"About","type":"page","target":"about","href":"/"},{"title":"Publications","type":"page","target":"publications","href":"/publications"},{"title":"CV","type":"page","target":"cv","href":"/cv"}],"siteTitle":"Jia Yang","enableOnePageMode":false}],["$","main",null,{"className":"min-h-screen pt-16 lg:pt-20","children":["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}],["$","$L6",null,{"lastUpdated":"November 28, 2025"}]]}]}]]}]]}],{"children":["__PAGE__",["$","$1","c",{"children":[["$","div",null,{"className":"max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-8 bg-background min-h-screen","children":["$","div",null,{"className":"grid grid-cols-1 lg:grid-cols-3 gap-12","children":[["$","div",null,{"className":"lg:col-span-1","children":["$","$L7",null,{"author":{"name":"Jia Yang","title":"M.S. Student","institution":"Nanjing University of Science and Technology","favicon":"/favicon.svg","avatar":"/bio.jpg"},"social":{"email":"jyang7@njust.edu.cn","location":"Nanjing, China","location_url":"https://maps.google.com","location_details":["Nanjing University of Science and Technology,","Nanjing, China"],"google_scholar":"https://scholar.google.com/citations?user=y6cWtTcAAAAJ&hl=zh-CN","orcid":"https://orcid.org/my-orcid?orcid=0009-0008-2903-9589","github":"https://github.com/xiangyuyyyy","linkedin":"https://linkedin.com"},"features":{"enable_likes":true,"enable_one_page_mode":false},"researchInterests":["Computational Physics","Mathematical Principles","Proof of Theorems"]}]}],["$","div",null,{"className":"lg:col-span-2 space-y-8","children":[["$","section","about",{"id":"about","className":"scroll-mt-24 space-y-8","children":[[["$","$L8","about",{"content":"Hi, I’m Jia Yang — a Master student in Computer Science and Technology at Nanjing University of Science and Technology. \n\nMy research interests lie in multimodal information extraction, visual grounding, and named entity recognition, where I aim to enhance machines’ ability to truly see and understand the real world from both language and visual modalities.","title":"About"}],["$","$L9","featured_publications",{"publications":[{"id":"yangvisual","title":"A Visual Annotation-Free Method That Rivals Fully Supervised Methods for Grounded Multimodal Named Entity Recognition","authors":[{"name":"Jia Yang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Jianfei Yu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zilin Du","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Wenya Wang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Li Yang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Rui Xia","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Boyang Li","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2025,"type":"journal","status":"published","tags":["Grounded Multimodal Named Entity Recognition","Visual Annotation-Free","Zero-shot Entity Visual Grounding","GradCAM"],"keywords":"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:0:tags","researchArea":"machine-learning","journal":"","conference":"","abstract":"Grounded Multimodal Named Entity Recognition (GMNER) aims to extract named entities, their types, and corresponding visual objects from image-text pairs. However, existing GMNER methods rely on costly multimodal annotations, limiting their scalability in real applications. To address this issue, we propose a visual annotation-free framework that leverages text-only NER data and a Zero-shot Entity Visual Grounding (ZeroEVG) approach. ZeroEVG consists of three modules: (1) Candidate Object Generation, which pre-selects visual object candidates; (2) Entity-Object Matching, which determines whether an entity has a visual presence; and (3) Entity Visual Localization, which employs a variant of GradCAM to identify bounding boxes for groundable entities. Experimental results on two benchmark datasets show that our visual annotationfree framework achieves competitive performance with fully supervised multimodal approaches, and even surpasses some of them under the same backbone on both GMNER and EVG tasks.","description":"A visual annotation-free method that rivals fully supervised methods for grounded multimodal named entity recognition, introducing a zero-shot entity visual grounding approach for visual annotation-free grounded multimodal named entity recognition.","selected":true,"preview":"TASLP2025.png","bibtex":"$a"},{"id":"yang2025annotating","title":"Annotating aspects in text and image: A new task and dataset for multimodal aspect-based sentiment analysis","authors":[{"name":"Li Yang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Jia Yang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Jin-Cheon Na","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Jianfei Yu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2025,"type":"journal","status":"published","tags":["Multimodal Aspect-Based Sentiment Analysis","Multimodal Named Entity Recognition","Visual Grounding","Large Language Model"],"keywords":"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:1:tags","researchArea":"machine-learning","journal":"Journal of the Association for Information Science and Technology","conference":"","abstract":"$b","description":"A new task and dataset for multimodal aspect-based sentiment analysis, introducing a visual aspect-aware multimodal large language model (VAM-LLM) for multimodal quadruple extraction.","selected":true,"preview":"JASIST2025.png","bibtex":"$c"},{"id":"li2024generative","title":"Generative Multimodal Data Augmentation for Low-Resource Multimodal Named Entity Recognition","authors":[{"name":"Ziyan Li","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Jianfei Yu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Jia Yang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Wenya Wang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Li Yang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Rui Xia","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2024,"type":"conference","status":"published","tags":["Multimodal Named Entity Recognition","Data Augmentation","Large Language Model","Stable Diffusion"],"keywords":"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:2:tags","researchArea":"machine-learning","journal":"","conference":"Proceedings of the 32nd ACM International Conference on Multimedia","pages":"7336--7345","abstract":"$d","description":"A generative multimodal data augmentation framework for low-resource multimodal named entity recognition, introducing a label-aware multimodal large language model (LMLLM) for multimodal text generation and a stable diffusion model for multimodal image generation.","selected":true,"preview":"MM2024.png","bibtex":"$e"}],"title":"Selected Publications","enableOnePageMode":true}],["$","$Lf","news",{"items":[{"date":"2023-09","content":"Starting my Master’s at Nanjing University of Science and Technology"}],"title":"News"}]],false,false,false]}]]}]]}]}],["$","$L10",null,{"children":"$L11"}],null,["$","$L12",null,{"children":["$L13","$L14",["$","$L15",null,{"promise":"$@16"}]]}]]}],{},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","JCLrF6NBsEi14ywTRQUK0",{"children":[["$","$L17",null,{"children":"$L18"}],null]}],null]}],false]],"m":"$undefined","G":["$19","$undefined"],"s":false,"S":true}
1a:"$Sreact.suspense"
1b:I[4911,[],"AsyncMetadata"]
11:["$","$1a",null,{"fallback":null,"children":["$","$L1b",null,{"promise":"$@1c"}]}]
14:null
18:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
13:null
1c:{"metadata":[["$","title","0",{"children":"Jia Yang"}],["$","meta","1",{"name":"description","content":"M.S. in Computer Science and Technology at Nanjing University of Science and Technology. "}],["$","meta","2",{"name":"author","content":"Jia Yang"}],["$","meta","3",{"name":"keywords","content":"Jia Yang,PhD,Research,Nanjing University of Science and Technology"}],["$","meta","4",{"name":"creator","content":"Jia Yang"}],["$","meta","5",{"name":"publisher","content":"Jia Yang"}],["$","meta","6",{"property":"og:title","content":"Jia Yang"}],["$","meta","7",{"property":"og:description","content":"M.S. in Computer Science and Technology at Nanjing University of Science and Technology. "}],["$","meta","8",{"property":"og:site_name","content":"Jia Yang's Academic Website"}],["$","meta","9",{"property":"og:locale","content":"en_US"}],["$","meta","10",{"property":"og:type","content":"website"}],["$","meta","11",{"name":"twitter:card","content":"summary"}],["$","meta","12",{"name":"twitter:title","content":"Jia Yang"}],["$","meta","13",{"name":"twitter:description","content":"M.S. in Computer Science and Technology at Nanjing University of Science and Technology. "}],["$","link","14",{"rel":"icon","href":"/fig.png"}]],"error":null,"digest":"$undefined"}
16:{"metadata":"$1c:metadata","error":null,"digest":"$undefined"}
